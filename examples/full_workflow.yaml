name: Complete Video Processing Workflow
description: Extract audio, split into segments, transcribe, format and correct transcription

# Input Configuration
# The input video file can be specified in three ways:
# 1. Via CLI using the -i flag: studioflowai run -w complete_workflow.yaml -i ./input/video.mp4
# 2. In the workflow file (first step's input parameter)
# 3. From a previous step's output in the workflow

# Output Configuration
# The output directory will be automatically created with timestamp:
# ./output/Complete_Video_Processing_Workflow-YYYYMMDD-HHMMSS/
# Each module's output will be stored in this directory

steps:
  - name: Extract Audio
    module: extractaudio
    parameters:
      # Input: Original video file
      input: ./tests/video-test.mov  # Can be overridden with -i flag
      # Output: Audio file in output directory
      outputName: "audio.wav"   # Will be stored in output directory
      sampleRate: 16000
      channels: 1
      
  - name: Transcribe Audio
    module: transcribe
    parameters:
      # Input: Audio file from previous step
      input: "${output}/audio.wav"  # References output directory
      # Output: Transcript file in output directory
      outputFileName: "transcript"  # Will create transcript.srt in output directory
      model: "whisper"
      # model: "whisper-cli"
      # whisperParams: "--model ./models/ggml-large-v3-turbo.bin --language auto --beam-size 4 --temperature 0.0 --best-of 3 --word-thold 0.01 --threads 4 --print-progress --output-srt --max-len 30 --audio-ctx 1500"
      # Language is optional - if not specified, Whisper will auto-detect
      # language: "Spanish"  # Uncomment to force a specific language
      outputFormat: "srt"    # Validated file extension
      whisperParams: "--model large-v3 --beam_size 5 --temperature 0.0 --best_of 5 --word_timestamps True --threads 16 --patience 1.0 --condition_on_previous_text True"

  - name: Format Transcription
    module: clean_text
    parameters:
      # Input: Transcript from previous step
      input: "${output}/transcript.srt"
      # Output: Cleaned transcript in output directory
      outputFileName: "transcript"
      removePatterns:
        - "Subt√≠tulos realizados por la comunidad de Amara\\.org"
      cleanFileSuffix: "_clean"
      preserveTimestamps: true
      preserveLineBreaks: true

  - name: Correct Transcription With ChatGPT
    module: correct_transcript
    parameters:
      # Input: Cleaned transcript from previous step
      input: "${output}/transcript_clean.txt"
      # Output: Corrected transcript in output directory
      outputFileName: "transcript_corrected"
      promptTemplate: "./prompts/transcription_correction.yaml"
      targetLanguage: "auto"
      model: "gpt-4o"
      temperature: 0.1
      maxTokens: 16384        # Maximum response tokens for GPT-4
      requestTimeoutMs: 300000  # 5 minutes timeout
      chunkSize: 120000        # Process in chunks of 120k tokens
      
  - name: Generate Social Media Content
    module: suggest_sns_content
    parameters:
      # Input: Corrected transcript from previous step
      input: "${output}/transcript_corrected.txt"
      # Output: Social media content in output directory
      outputFileName: "social_media_content"
      model: "gpt-4o"
      temperature: 0.8  # Slightly higher temperature for creative content
      maxTokens: 16000   # Increased for more detailed content
      language: "Spanish"  # Generate content in Spanish
      promptFilePath: "./prompts/sns_content.yaml"  # Custom prompt template file (optional)
      
  - name: Generate Shorts Suggestions
    module: suggest_shorts
    parameters:
      # Input: Original transcript for timing information
      input: "${output}/transcript.srt"
      # Output: Shorts suggestions YAML in output directory
      outputFileName: "shorts_suggestions"
      model: "gpt-4o"
      temperature: 0.9                             # Higher temperature for creative suggestions
      maxTokens: 16000
      minDuration: 45                              # Minimum clip duration in seconds
      maxDuration: 75                              # Maximum clip duration in seconds
      promptFilePath: "./prompts/shorts_prompts.yaml"  # Quality-focused prompt template
      requestTimeoutMs: 300000  # 5 minutes
      chunkSize: 120000        # Adjust based on your needs
      
  - name: Extract Shorts Clips
    module: extract_shorts
    parameters:
      # Input: Shorts suggestions and original video
      input: "${output}/shorts_suggestions.yaml"
      videoFile: "./tests/video-test.mov"  # Original input video file
      # Output: Shorts clips in output/shorts directory
      ffmpegParams: "-vf scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2,setsar=1 -c:v libx264 -c:a aac -b:a 128k -b:v 2500k"
      quietFlag: true
      
  - name: Add Text Overlay
    module: set_title_to_short_video
    parameters:
      # Input: Shorts suggestions and original video
      input: "${output}/shorts_suggestions.yaml"
      videoFile: "./tests/video-test.mov"  # Original input video file
      # Output: Shorts with text in output/shorts_with_text directory
      fontSize: 50
      fontColor: "white"
      boxColor: "black@0.5"
      boxBorderW: 5
      quietFlag: true
      fontFile: "/System/Library/Fonts/Helvetica.ttc"  # Native macOS font
      textX: "(w-text_w)/2"
      textY: "(h/3)-50"